题目：面向深度学习的一种基于ERA的任务调度系统的设计与实现

# 摘要

进入21世纪10年代以来，深度学习和云计算在IT产业中占据了越来越重要的地位。在深度学习这一垂直领域，大多现有的云端系统，仅仅提供了基础设施级别的服务，而深度学习开发环境的搭建以及运维工作相对繁杂，需要更为定制化的云端系统。此外，现有云端系统的资源利用率仍然有较大的提升空间。

针对这一现状，本课题以面向深度学习领域提供平台级服务为目标，并且将该云端系统的核心问题定义为任务调度，设计并实现了面向深度学习的一种基于ERA的任务调度系统。所谓ERA（Economic Resource Allocation），是斯坦福和微软学者在[ref]一文中所介绍的基于预定这一经济学原则、适用于云计算的资源分配框架。本课题将ERA框架融入到任务调度方案的设计之中，并且运用了Docker容器集群和云存储等云计算技术，在经济学和系统学这两个领域，对深度学习的任务调度解决方案做了有益尝试。

本文所述系统在仿真平台上通过了各项测试，并且在真实的深度学习运维平台上部分得以应用，充分证明了其有效性。

# 1. 绪论
## 1.1 研究背景及意义
近年来，随着计算机单机计算及分布式计算能力的大幅提升，以神经网络算法为代表的深度学习取得了飞跃式发展，开始更广泛地应用于人工智能各个领域，促进了计算机视觉、数据挖掘、语音识别、自然语言处理等交叉学科的迅猛发展，成为近些年来人工智能领域最受瞩目的领域。不仅高校、企业通过校企合作、成立研究院、实验室等广泛开展深度学习研究，由于一些深度学习框架的开源和数据开放，传统信息技术领域也涌现了许多深度学习爱好者。但是，搭建深度学习平台的成本相当高昂，流程相对复杂，具有一定的工程技术要求。硬件方面，由于深度学习研究对计算能力的高要求，需要购买高性能GPU、CPU、SSD、主机等；软件方面，通常需要搭建多种深度学习框架，并构建作业提交机制。此外，以高校实验室为代表机构所构建的深度学习私有云平台，由于共享机制较为原始，资源利用率较低[1]。因此，如何提出一套面向深度学习的云端系统解决方案，简化其研究和开发流程，规范化、自动化深度学习研究领域的运营和维护工作，并有效提高资源利用率以降低经济成本，将成为深度学习研发过程中面临的一个重要问题。

深度学习平台的云端化，已成为业内共识和普遍趋势。所谓云端化，依托于云计算的快速发展。云计算，是指计算资源（如CPU，GPU，存储等）作为可以出租的通用工具，按需提供给用户租用。云计算的出现对整个信息技术行业产生了巨大的影响，谷歌、亚马逊、微软、阿里巴巴等大公司试图成为类似电信运营商的云计算服务提供商，提供更强大、更可靠、更低成本的云计算平台，并且已在互联网领域取得了相当多的最佳实践。
云计算服务通常意义上分为三类：基础设施即服务（IaaS，infrastructure as a service），为用户提供硬件设施（服务器、存储、网络）和相关软件（虚拟化操作系统、文件系统）服务；平台即服务（PaaS, platform as a service），通过Web向开发人员提供应用程序开发和部署平台服务；软件即服务（SaaS, software as a service），为用户提供托管的软件（运行在平台和基础设施上）。

## 1.2 国内外研究现状
随着虚拟化技术的发展和云计算技术的日益成熟，越来越多的企业专注于为企业用户乃至个人用户提供PaaS服务。一些 PaaS 供应商使用容器来减少为每个应用程序创建一个新的虚拟机的开销, 从而降低了运行 PaaS 应用程序的成本, 同时在进程、网络和文件系统级别保持隔离，保证了可移植性和安全性。相比于IaaS，PaaS专注于垂直领域，更趋向定制化。而深度学习的发展历程较短，发展速度较快，可谓日新月异，因此还未出现一家成熟的PaaS服务商，为深度学习研究者提供应用级服务。深度学习研究者只能使用IaaS服务商提供的云计算资源，按图索骥一步步搭建定制化的深度学习环境。这一过程中，开发者不得不投入时间和精力来管理计算资源、搭建相应软件环境、部署应用程序。随着深度学习研究与应用的普及，部分云计算企业了发布深度学习领域的PaaS平台，为深度学习研究者提供更精细化的服务。目前出现的深度学习PaaS平台主要有以下三类： 
（1）提供训练环境以及周边功能，能够将深度学习模型快速转化为应用。
（2）仅提供深度学习云计算服务，提供了基本的快速部署和训练深度学习模型训练功能。
（3）提供定制化的深度学习环境，提供某种特定深度学习框架的软件环境。
	以上现有的平台并不能解决目前深度学习研发所面临的问题，包括FlyodHub、TinyMind、RussellCloud等在内的该领域初创企业的产品，在可用性、易用度和性价比（由云资源利用率反映）方面仍然有很大的优化空间。而Azure和Bluemix提供的深度学习平台的关键弱点是可扩展性，与开源深度学习框架的集成度较低，且在云资源利用率方面乏善可陈。


## 1.3 论文结构
本文全文结构如下：
第一章，绪论，主要介绍了本文所涉及研究领域的现状及动态，包括云计算的各类形式和层次、工业届的深度学习作业工作流以及任务调度方案、目前所面临的问题等，在本章最后提出了解决方案：“面向深度学习的一种基于ERA的任务调度系统”，并对其作了简要描述。

第二章，相关技术介绍。介绍了与本文所设计并实现的系统相关度较高的几项关键技术。

第三章，需求分析。从任务调度与资源分配这一系统学领域，以及深度学习这一垂直细分的业务领域，对本系统旨在满足的需求进行了完整分析，且提出了检验本系统的一些标准。

第四章，设计与实现。本章首先提出了一个层次分明的总体架构，接下来进行分层设计，对各层的功能和性能提出设计要求，然后再提出设计方案，并根据该设计方案给出具体实现方案。

第五章，测试与分析。本章记录了第四章中所论述实现的系统的实际运行测试结果，包括功能和性能两方面的，并结合第二章所提出的检验标准进行针对性分析。

第六章，总结与展望。本章给出现有阶段下本系统的不足，对未来的进一步优化进行展望。

# 2. 相关技术介绍

## 2.1 docker相关技术介绍
### 2.1.1 虚拟化技术概览
虚拟化（Virtualization）技术最早出现在 20 世纪 60 年代的 IBM 大型机系统，在70年代的 System 370 系列中逐渐流行起来，这些机器通过一种叫虚拟机监控器（Virtual Machine Monitor，VMM）的程序在物理硬件之上生成许多可以运行独立操作系统软件的虚拟机（Virtual Machine）实例。随着近年多核系统、集群、网格甚至云计算的广泛部署，虚拟化技术在商业应用上的优势日益体现，不仅降低了 IT 成本，而且还增强了系统安全性和可靠性，虚拟化的概念也逐渐深入到人们日常的工作与生活中。

虚拟化技术主要分为以下几个大类 [ref]：
- 平台虚拟化（Platform Virtualization），针对计算机和操作系统的虚拟化。
- 资源虚拟化（Resource Virtualization），针对特定的系统资源的虚拟化，比如内存、存储、网络资源等。
- 应用程序虚拟化（Application Virtualization），包括仿真、模拟、解释技术等。

我们通常所说的虚拟化主要是指平台虚拟化技术，通过使用控制程序（Control Program，也被称为 Virtual Machine Monitor 或 Hypervisor），隐藏特定计算平台的实际物理特性，为用户提供抽象的、统一的、模拟的计算环境（称为虚拟机）。虚拟机中运行的操作系统被称为客户机操作系统（Guest OS），运行虚拟机监控器的操作系统被称为主机操作系统（Host OS），当然某些虚拟机监控器可以脱离操作系统直接运行在硬件之上（如 VMWARE 的 ESX 产品）。运行虚拟机的真实系统我们称之为主机系统。

![图：虚拟化技术的分类](http://opkk27k9n.bkt.clouddn.com/18-4-27/93911199.jpg)

实现 IT 基础架构的虚拟化可以带来以下好处：
1. 效率：将原本一台服务器的资源分配给了数台虚拟化的服务器，有效的利用了闲置资源，确保企业应用程序发挥出最高的可用性和性能。
2. 隔离：虽然虚拟机可以共享一台计算机的物理资源，但它们彼此之间仍然是完全隔离的，就像它们是不同的物理计算机一样。因此，在可用性和安全性方面，虚拟环境中运行的应用程序之所以远优于在传统的非虚拟化系统中运行的应用程序，隔离就是一个重要的原因。
3. 可靠：虚拟服务器是独立于硬件进行工作的，通过改进灾难恢复解决方案提高了业务连续性，当一台服务器出现故障时可在最短时间内恢复且不影响整个集群的运作，在整个数据中心实现高可用性。
4. 成本：降低了部署成本，只需要更少的服务器就可以实现需要更多服务器才能做到的事情，也间接降低了安全等其他方面的成本。
5. 兼容：所有的虚拟服务器都与正常的x86系统相兼容，他改进了桌面管理的方式，可部署多套不同的系统，将因兼容性造成问题的可能性降至最低。
6. 便于管理：提高了服务器/管理员比率，一个管理员可以轻松的管理比以前更多的服务器而不会造成更大的负担。

### 2.1.2 Docker简介
Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护（项目地址：https://github.com/moby/moby）。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。

Docker公司的创始人兼CTO——Solomon Hykes，有机地把一系列技术Cgroups、Namespace和UnionFS整合起来，极大地降低了容器技术的复杂度，提升了开发者的用户体验。他敏锐地预测到，一旦标准化容器技术最终出现，整个技术行业将会受到深远的影响。Docker公司开源了Docker Engine，定义了一个以容器镜像为标准的应用打包格式，并且建立Docker Hub服务（https://hub.docker.com）进行镜像分发和协作。这些举措迅速创建了一个良好的社区和合作伙伴生态圈，包含AWS、Google、Microsoft、IBM和国内的众多公司。在短短几年的时间内，Docker几乎成为了容器技术的代名词。

Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目已经超过48000个star和14000个fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。

Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。

Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。

传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。

Docker与传统虚拟化技术的比较：
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/77539209.jpg)
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/92916237.jpg)


### 2.1.3 基本概念：镜像、容器和仓库
docker 包括三个基本概念：
- 镜像（Image）
- 容器（Container）
- 仓库（Repository）
#### 2.1.3.1 Docker 镜像
我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。

Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。

Docker镜像具有分层存储的特性。因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。

镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。

分层存储的特征还使得镜像的复用、定制变得更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。

![](http://opkk27k9n.bkt.clouddn.com/18-5-4/42272106.jpg)
#### 2.1.3.2 Docker容器
镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。

容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。

前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。

容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。

按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。

数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。
#### 2.1.3.3 Docker Registry
镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。

一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。

通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 <仓库名>:<标签> 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。

以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。

仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。

Docker Registry 公开服务：
Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。

最常使用的 Registry 公开服务是官方的[Docker Hub](https://hub.docker.com/)，这也是默认的 Registry，并拥有大量的高质量的官方镜像。

除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。

### 2.1.4 Docker Compose
Compose是定义和运行多容器Docker应用程序的工具。 使用Compose，我们可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，创建并启动配置中的所有服务。

使用Compose基本上是一个三步过程：
1. 使用Dockerfile定义应用程序的环境，以便在任何地方进行复制。
2. 在docker-compose.yml中定义组成应用程序的服务，以便它们可以在隔离的环境中一起运行。
3. 运行docker-compose并撰写开始并运行整个应用程序。

Compose有以下特性：
- 单个主机上有多个独立的环境
- 创建容器时保留卷数据
- 只重新创建已更改的容器：Compose缓存用于创建容器的配置。重新启动未更改的服务时，Compose将重新使用现有的容器，这意味着我们可以快速更改环境。
- 支持Compose文件中的变量：可以使用这些变量为不同的环境或不同的用户自定义组合。

### 2.1.5 容器集群管理工具介绍
#### 2.1.5.1 Docker Swarm
在Docker 1.12 之前，独立版的Docker Swarm作为Docker的原生集群系统，使用API代理系统将Docker主机池变成单个虚拟主机。它是2014年开始的Docker第一个容器编排项目。结合Docker Compose，这是一个用来安排容器的非常方便的工具。其灵活性和简单性使其易于与现有IT基础架构集成。

在Docker 1.12及更高版本中，Swarm模式已经集成到Docker Engine中。Swarmkit是Docker Engine 1.12或更高版本中的集群管理和编排功能。当启用Swarmkit时，我们调用Docker Engine以Swarm模式运行。

Swarm模式内置 kv 存储功能，提供了众多的新特性，比如：具有容错能力的去中心化设计、内置服务发现、负载均衡、路由网格、动态伸缩、滚动更新、安全传输等。使得 Docker 原生的 Swarm 集群具备与 Mesos[ref]、Kubernetes 竞争的实力。

swarm mode特性介绍：
1. 与Docker Engine集成的集群管理：使用Docker Engine CLI创建一群Docker引擎，您可以在其中部署应用程序服务。 您不需要额外的编排软件来创建或管理群。
2. 声明式服务模型：Docker Engine使用声明式方法让您在应用程序堆栈中定义各种服务的所需状态。例如，您可能会描述一个由带有消息队列服务和数据库后端的Web前端服务组成的应用程序。
3. 缩放：对于每个服务，您可以声明要运行的任务数量。当您向上或向下缩放时，s​​warm管理器会通过添加或删除任务来自动调整以保持所需的状态。
4. 期望的状态协调：swarm manager节点持续监视集群状态，并协调实际状态与表达期望状态之间的任何差异。例如，如果您设置了一个服务来运行一个容器的10个副本以及一个承载其中两个副本崩溃的工作机，则该管理器会创建两个新副本来替换发生崩溃的副本。 swarm manager将新副本分配给正在运行且可用的工作人员。
5. 多主机联网：您可以为您的服务指定覆盖网络。 swarm管理器在初始化或更新应用程序时自动为覆盖网络上的容器分配地址。
6. 服务发现：Swarm管理器节点为swarm中的每个服务分配一个唯一的DNS名称并负载平衡正在运行的容器。您可以通过群集中嵌入的DNS服务器查询群集中运行的每个容器。
7. 负载平衡：您可以将服务的端口暴露给外部负载平衡器。在群集内部，您可以指定如何在节点之间分发服务容器。
8. 安全：群中的每个节点都强制进行TLS相互认证和加密，以保护其自身与所有其他节点之间的通信。可以选择使用自定义根证书或来自自定义根CA的证书。
9. 滚动更新：在推出时，您可以逐步将服务更新应用于节点。 swarm管理器允许您控制服务部署到不同节点集之间的延迟。如果出现任何问题，您可以将任务回滚到以前版本的服务。

#### 2.1.5.2 Kubernetes
Kubernetes 是 Google 团队发起并维护的基于 Docker 的开源容器集群管理系统，它不仅支持常见的云平台，而且支持内部数据中心。

建于 Docker 之上的 Kubernetes 可以构建一个容器的调度服务，其目的是让用户透过 Kubernetes 集群来进行云端容器集群的管理，而无需用户进行复杂的设置工作。系统会自动选取合适的工作节点来执行具体的容器集群调度处理工作。其核心概念是 Container Pod。一个 Pod 由一组工作于同一物理工作节点的容器构成。这些组容器拥有相同的网络命名空间、IP以及存储配额，也可以根据实际情况对每一个 Pod 进行端口映射。此外，Kubernetes 工作节点会由主系统进行管理，节点包含了能够运行 Docker 容器所用到的服务。

Kubernetes具有以下特性：
1. 自动装箱：根据资源需求和其他限制自动放置容器，同时不牺牲可用性。混淆重要和尽力服务的工作负载，以提高利用率并节省更多资源。
2. 水平缩放：使用一个简单的命令，使用UI或根据CPU使用情况自动扩展和缩小您的应用程序。
3. 自动发布和回滚：Kubernetes逐步推出对应用程序或其配置的更改，同时监视应用程序运行状况以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您回滚更改。 充分利用日益增长的部署解决方案生态系统。
4. 存储编排：自动安装所选择的存储系统，无论是从本地存储，公共云提供商（如GCP或AWS）还是网络存储系统（如NFS，iSCSI，Gluster，Ceph，Cinder或Flocker）。
5. 自愈：重新启动失败的容器，在节点死亡时替换容器并重新安排容器，杀死对用户定义的运行状况检查无响应的容器，并且在准备好提供服务之前不会将它们通告给客户端。
6. 服务发现和负载均衡：无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes为容器提供了自己的IP地址和一组容器的单个DNS名称，并且可以在它们之间进行负载平衡。
7. 密钥和配置管理：部署和更新秘密和应用程序配置，无需重新构建映像，也不会在堆栈配置中暴露密钥。
8. 批量执行：除了服务之外，Kubernetes还可以管理批处理和CI工作负载，并根据需要替换失败的容器。

![](http://opkk27k9n.bkt.clouddn.com/18-4-27/19557914.jpg)

## 2.2 分布式异步任务调度技术介绍
任务调度是计算环境中的一个重要特性。在本文所述的系统中，要执行的任务主要是深度学习计算任务。

当前的单机任务系统，意味着任务列表（或调度数据）是本地存储的，并位于执行任务的同一台计算机上。虽然这些系统在单个计算机环境中运行良好，但是可扩展能力有重要缺陷。例如，如果计划在同一时间或几乎同时运行多个任务，单主机可能会很快变得负担过重或资源不足，结果导致某些任务可能无法正确执行。因此，分布式的任务调度已经成为云计算领域的关键技术之一，我们需要一套将任务管理与任务执行分开的分布式任务调度方法和系统：其中任务管理和任务运行在独立的计算设备上执行，而任务管理由至少一个数据中介执行，而任务执行由多个执行主机执行。对于此类系统，消息的分布式传递是其核心内容之一。

### 2.1 Celery简介
Celery是基于分布式消息传递的异步任务队列/作业队列。 它专注于实时操作，但也支持调度。执行单元（称为任务）在使用多处理、Eventlet[ref]或gevent[ref]的单个或多个工作服务器上同时执行。 任务可以异步执行（在后台）或同步执行（等待直到准备就绪）。

![](http://opkk27k9n.bkt.clouddn.com/18-4-16/1740887.jpg)

Celery具有以下特性：
1. 易于集成：Celery很容易与Web框架集成，其中一些甚至包含集成包。Celery是用Python编写的，但协议可以用任何语言实现。 它也可以使用webhooks[ref]与其他语言一起运行。
2. 支持多消息代理
推荐的消息代理是RabbitMQ[ref]，但也支持Redis，Beanstalk[ref]，MongoDB[ref]，CouchDB[ref]和数据库（使用SQLAlchemy或Django ORM）。

Celery的消息协议支持的关键参数包括以下内容：
- task 客户端要求调用的函数名称，必须是已注册到Celery Worker进程中的函数。
- args 调用函数的参数列表
- eta 客户端要求在该时刻调用函数。
- expires 任务调度消息的过期时间。

任务消息头部的content_type字段还可以指定一些序列化格式，默认支持的MIME类型见下表：

|Scheme	|MIME| Type|
|---|---|---|
|json	|application/json|
|yaml	|application/x-yaml|
|pickle	|application/x-python-serialize|
|msgpack|application/x-msgpack|

目前Celery的最新版本4.1版本默认支持json的序列化格式。

## 2.3 数据库介绍
### 2.3.1 Redis
Redis是一个开源的（BSD许可）内存数据结构存储，可用作数据库、缓存和消息代理。它支持的数据结构，如字符串，散列，列表，集合，具有范围查询的排序集，位图，超级日志和具有半径查询的地理空间索引。 Redis具有内置复制，Lua脚本，LRU驱逐，事务和不同级别的磁盘持久性，并通过Redis Sentinel提供高可用性，并通过Redis集群实现自动分区。[ref]

Redis经常被称为数据结构服务器。这意味着Redis通过一组命令提供对可变数据结构的访问，这些命令是使用带有TCP套接字和简单协议的服务器 - 客户机模型发送的。因此，不同的进程可以以共享的方式查询和修改相同的数据结构。

Redis中实现的数据结构有一些特殊的属性：
- Redis会将它们存储在磁盘上，即使它们始终被提供并修改到服务器内存中。这意味着Redis速度很快，但也是非易失性的。
- 数据结构的实现强调内存效率，因此与使用高级编程语言建模的相同数据结构相比，Redis内部的数据结构可能会使用更少的内存。
- Redis提供了很多在数据库中很自然的功能，如复制，可调节级别的耐用性，集群，高可用性。

另一个很好的例子是将Redis看作memcached的更复杂版本，其中操作不仅仅是SET和GET，而是用于处理像列表，集合，有序数据结构等复杂数据类型的操作。

### 2.3.2 MySQL
MySQL 是一种快速易用的 RDBMS，很多企业（不分规模大小）都在使用它来构建自己的数据库。MySQL 由一家瑞典公司 MySQL AB 开发、运营并予以支持。MySQL是一种关系型数据库管理系统（RDBMS），支持标准的结构化查询语言（Structured Query Language）。

MySQL具有以下特性[ref]：
1. 使用C和C++编写，保证了源代码的可移植性。
2. 支持多种操作系统。
3. 为多种編程语言提供了API。
4. 支持多线程，充分利用CPU资源，支持多用户。
5. 优化的SQL查询算法，有效地提高查询速度。
6. 既能够作为一个单独的应用程序在客户端服务器网络环境中运行，也能够作为一个程序库而嵌入到其他的软件中。
7. 对多种字符集的完全支持。
8. 提供TCP/IP、ODBC和JDBC等多种数据库连接途径。
9. 提供用于管理、检查、優化数据库操作的管理工具。
10. 可以处理拥有上千万条记录的大型数据库。

## 2.4 本章小结
总览本章，2.1节着重介绍了docker相关的容器化技术栈，docker技术栈是本文所论述的任务调度系统所依赖的底层核心组件；2.2节介绍了Celery，Celery以及Celery所实践的分布式任务系统架构，在本系统中得以应用，负责任务调度消息的分布式分发；2.3节介绍了Redis和MySQL这两种数据库。

# 3. 需求分析

## 3.1 深度学习业务需求分析

本系统的应用领域是深度学习，典型用户是深度学习开发者。在目前的技术环境下，深度学习开发者在进行深度学习研发之前，通常需要搭建深度学习环境。

在一个典型的深度学习环境搭建过程中，用户一般需要完成以下三个环节：
(1)	购买或者租用相应的硬件设施，包括服务器、数据存储器、GPU等设备。
(2)	配置、组装上述设备，使其构成完整的计算工作流，包括操作系统的安装、网络管理和数据盘的挂载等。
(3)	搭建选定的深度学习软件环境，安装相应的软件包、深度学习框架以及必要的通用开发工具等。

完成环境搭建后，用户在搭建好的环境中开始执行深度学习作业之前，由于深度学习作业通常需要大型数据集作为训练数据，因此需要事先下载这些数据集。完成下载后，用户手动启动作业的执行，在这一过程中用户通常会查看作业日志（一般是控制台标准输出，或者是可视化处理过的），以及作业运行的资源占用情况。深度学习作业执行结束后，可能会生成一些模型文件或其他数据文件，用户需要保存这些“任务输出”。

结合如上所述完整的深度学习作业流程，在面向深度学习领域的任务调度系统中，我们需要：

1. 以GPU为代表的硬件支持；
2. 以数据集为代表的海量数据存储支持，以及公有数据集和私有数据集的权限分离；
3. 深度学习软件环境的多样化支持和统一配置；
4. 深度学习作业执行过程的日志监控和性能监控；
5. 保存作业的输出结果。

此外，为了面向用户构建系统，提高系统易用性与可用性，系统需要提供一些必要服务。就具体服务内容而言，我们需要：

- 核心应用服务
  主要提供用户接入，包括登入登出、提交作业、查看作业等。此外，该服务为系统管理员提供系统配置接口。
- 用户文件服务
  在本文所述深度学习场景下，用户文件主要包括代码文件以及数据文件。其中，代码文件较小，而数据文件则相对较大。用户为执行一项作业，需要上传这项作业中需要使用的这些文件。
- 用户作业监控服务
  完成作业运行时关键指标监控、日志监控、深度学习训练过程可视化等，并提供相关访问接口。关键指标包括CPU占用率、内存占用率、磁盘I/O、网络I/O等。此外，深度学习训练过程可视化也是需求之一。
- 镜像定制发布服务
  该服务主要针对本系统的运维管理人员，针对当下深度学习框架层出不穷、版本更新快的特点，实现深度学习框架的镜像化定制与发布，以满足用户进行深度学习研究的多样化需求。当然，用户也可以借由该服务实现自定义镜像，实现高扩展性。
- 数据可视化服务
  不同于训练过程可视化，本平台提供的数据可视化服务主要针对用户在本平台存储的数据集和作业输出数据。该服务将基于Jupyter Notebook及其扩展，构建中心化的服务器集群以提供数据可视化服务。

从流行的微服务观点看[ref]——微服务架构是一种以一些微服务来替代开发单个大而全应用的方法, 每一个小服务运行在自己的进程里,并以轻量级的机制来通信, 通常是 HTTP RESTful API. 微服务强调小快灵, 任何一个相对独立的功能服务不再是一个模块, 而是一个独立的服务。因此，用户需要一些交互工具来使用这些服务——提供命令行工具，或者一个Web站点，以便用户使用这些服务。

## 3.2 任务调度与资源分配
[ref] https://en.wikipedia.org/wiki/Scheduling_computing

在计算中，调度是通过某些方法指定的工作分配给完成工作的资源的方法。这项工作可能是虚拟计算元素，例如线程，进程或数据流，这些虚拟计算元素又被调度到诸如处理器，网络链接或扩展卡之类的硬件资源上。

调度程序是执行调度活动的内容。调度程序经常需要保持所有计算机资源的繁忙（一个经典的术语是负载均衡），允许多个用户有效地共享系统资源，或者实现目标服务质量。

调度器可以聚焦于一个或多个目标，例如：

1. 最大化吞吐量（每个时间单元完成的工作总量）;
2. 最大限度地减少等待时间（从工作开始到第一个点开始执行资源的时间）;
3. 最大限度地减少等待时间或响应时间（从批处理活动开始直到完成工作为止的时间，或者直到系统响应并在交互活动时将第一个输出交给用户）; 
4. 最大化公平性（每个流程的CPU时间相同，或者根据每个流程的优先级和工作负载更一般地适当的时间）。

实际上，这些目标经常发生冲突（例如吞吐量与延迟），因此调度程序不得不做出一些妥协——根据用户的需求和目标，优先考虑上述任何一个问题。

从系统学角度讲，任务调度事实上就是调度器（例如操作系统内核）对诸如CPU、内存等计算机资源的分配过程。[ref]在本文后续内容中，将不再对这两个词作特别的区分。

在本系统中，任务调度的对象是深度学习作业，任务调度的目标是容器集群，资源分配的内容包括：

1. CPU核心数
2. 内存大小
3. GPU显卡数量
4. 外存大小

容器技术所使用的Cgroups技术，事实上已经涉及到资源分配，这是由Linux内核提供的进程级资源分配能力；容器集群的管理工具，进一步实现了更高层级的资源分配[ref]。

在本系统中，由于集成了预定这一经济学准则，还需要对资源进行进一步的建模。一个具有经济行为能力的控制中心（例如资源定价模块），需要通晓整个系统中的资源总量、使用情况以至未来需求，以实现现实世界中的浮动定价机制，发挥“市场配置资源”的作用（市场配置资源指的是经济运行过程中，市场机制根据市场需求与供给的变动引起价格变动从而实现对资源进行分配，组合及再分配与再组合的过程。），从而实现资源配置的高效性。

## 3.3 检验标准
针对3.1节和3.2节所述需求，本文所设计的系统需要满足以下要求：

- 提供适用于个人桌面计算机的客户端软件，功能包括根据用户输入发起深度学习作业请求、查看作业日志等。
- 稳定运行的服务端软件，可以接受作业请求并准确执行，包括作业的起始时间、硬件资源分配，以及其他作业描述信息。


- 根据云资源利用率计算公式，接入核心调度层之后云资源利用率有提高。

云资源利用率的计算公式如下：

静态集群的资源利用率=Sum(集群中某机器的CPU占用率在一段时间内的均值 * cpu_ratio + GPU占用率 * gpu_ratio + 内存占用率 * mem_ratio)/某集群机器总数，其中cpu_ratio + gpu_ratio + mem_ration = 1。

对于动态伸缩的集群，该公式在计算的时间粒度上作相应调整即可。

## 3.4 本章小结
本章从任务调度系统本身，以及本系统所在的垂直细分领域——深度学习这一角度，阐述了本系统所需要满足的一些基本需求，并对这些需求的满足提出了检验标准。

本文将在下一章就如何满足本章提出的需求，提出系统的设计方案。

# 4. 设计与实现
本章将全面阐述本系统的设计方案，以及关键实现。

采用自顶向下的设计思路，可以提出如下关键架构：

![](http://opkk27k9n.bkt.clouddn.com/18-4-30/73245956.jpg)

其中，用户接入层直接面向本系统的普通用户（即深度学习研发者），作为一个用户接入的交互工具而存在；核心应用层是本系统的”中枢神经“，提供各类服务接口（如3.1节所述服务内容）；核心调度层是ERA框架[ref]在本系统中的集成，接受核心应用层的调度请求，通过一系列算法模块，决定具体调度方案，并将调度指令发送给基础设施层；基础设施层是本系统的底层基建，作为通常意义上的云端系统，完成实际的任务调度。

该架构的各个层级，可以作为单独的软件部署，即本系统从层级来看是分布式的。而本系统作为一个整体的分布式系统，需要一定的信息共享通信机制。因此，在这四个层级之外，有一个四个层都可以访问的信息共享组件。下文不再单独介绍。

本系统的应用架构如下：

![](http://opkk27k9n.bkt.clouddn.com/18-5-1/30546676.jpg)

下面将就如图所示架构设计，自下而上地展开叙述。

## 4.1 基础设施层

本系统中，基础设施层是对云计算服务的一种封装——这一云计算服务可以是公有云服务商所提供的，也可以是自建的私有云服务，同时也体现了本系统的关键设计理念。由于这些云服务是关键性的底层依赖，对其进行的封装设计可以使得整个系统架构设计具有高内聚低耦合的特性。基础设施层的设计，基本思路是：

- 计算与存储隔离（灵活配置）
- 计算与计算隔离（安全可靠）
- 存储与存储隔离（权限可分）

### 4.1.1 设计目标
### 4.1.1.1 设计职能目标

基础设施层的基本职能简单来说就是，执行指定任务，并且提供查看任务执行情况的接口。就其在本系统的定位而言，基础设施层接受核心调度层的调度指令，并予以执行。

### 4.1.1.2 设计性能目标

基础设施层接受核心调度层的调度指令之后，以及实际的作业开始执行之前，将经历一系列的处理流程，这一过程称之为”作业配置“，我们将作业配置所消耗的时长，作为基础设施层的关键性能衡量指标。理论上我们希望这一时长越短越好。

### 4.1.2 设计方案
基础设施层对关键技术的选型如下：
1. 计算方面，选用Docker Engine内置的swarm mode集群。Swarm mode 集群的架构图如下所示:
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/11857606.jpg)
2. 存储方面，选用NAS存储。NAS可以方便地挂载于云服务器（使用mount命令，nfs4协议）、Docker容器（Docker原生支持的存储协议），且支持权限组，可以实现文件系统级别的读写权限分离。
3. 在计算集群所在局域内网搭建私有Docker Registry，以加速镜像拉取。

理论上，基础设施层也是可以运行于私有云之上的。这源于下述基础设施管理接口的封装设计（为论述方便，将这部分封装设计称为“云接口”）：
1. 从“已排期作业队列”中读取作业标识符（jobId）；
2. 根据jobId，从数据库读取相关作业描述信息（jobDesc）；
3. 根据jobDesc，执行作业。具体执行过程包括（下述过程均封装为可调用接口；在本系统中，“作业进程”是一个或多个具体的容器）：
    - 申请硬件资源；
    - 启动作业进程；
    - 监控作业进程；
    - 回收作业进程。
4. 上述执行过程，均通过调用集群管理器接口实现（将jobDesc集成到调用参数中，通常以yaml的格式）。集群管理器接口包括以下关键内容：
    - 创建作业实例
    - 查看作业实例
    - 启动作业实例
    - 终止作业实例
    - 删除作业实例
    - 创建数据卷
    - 删除数据卷

参考分布式任务队列Celery的架构，有几大关键子模块：
- user 
- broker
- worker
- backend

在本系统的设计中，云接口对应上述几个子模块相应地有：

- broker是云接口对外（对user）暴露的作业调度信息接入点。
- worker依据broker中的作业调度信息，执行作业并追踪作业的整个生命周期（即“监控作业进程”）。
- backend是由接口调用者指定的，被worker用于存储作业的相关信息。

### 4.1.3 实现方案
#### 4.1.3.1 运行环境

1. 操作系统：主流现代类Unix系统，包括Linux/OS X。
2. 软件依赖：
   1. Docker
   2. Python（包括Flask, Celery等第三方包）
   3. Redis
   4. MySQL
   5. 阿里云容器服务

#### 4.1.3.2 关键实现
本系统对于基础设施层的实现，表现为"基于阿里云容器集群服务的Celery应用"，即：选用阿里云提供的容器服务作为基础服务；以Python作为主要编程语言，主要使用Celery软件包构建应用程序（Celery介绍见2.2节），同时使用阿里云提供的Python SDK实现云服务调用。

本节将详细介绍以下内容：
1. 根据作业描述对作业进行"实例化"的过程。
2. "追踪作业整个生命周期"的过程细节。
3. "实例化"过程所依赖的容器的镜像持续集成方案。

1）作业”实例化“过程

作业的执行，被封装到DoJob函数中，DoJob函数被注册为Celery Task，因此可以通过Celery实现该函数的远程调用。DoJob函数的伪代码如下所示：

```python
def DoJob(jobId):
    aliyun_app = Application(job_id)
    return aliyun_app.Run()
```

其中，Application类是我们对用户作业进行的面向对象程序设计，传入作业ID执行相应的初始化流程，主要包括从数据库查询并加载作业相关描述信息、用户信息；将作业状态从“等待中”更新为“配置中”（作业状态由核心应用层维护，这可以看做是对核心应用层的“反射”）。接下来的Run()方法，封装了一系列方法，流程如下图所示：

![](http://opkk27k9n.bkt.clouddn.com/18-5-1/95693872.jpg)

事实上，作业的“实例化过程”，主要包括图所示流程中的以下步骤：

1. 初始化，过程如前文所述。

2. 检查集群资源，首先查询阿里云集群的资源闲置情况，如果没有足够的空闲资源，将调用阿里云API触发集群扩容。这一步是堵塞的，如果触发了扩容操作，耗时将会比较长。

3. 拷贝用户代码，用户作业指定了某代码版本，可认为该代码所在文件夹是只读的，这里需要将其拷贝到另一个可读写文件夹，以便随后挂载到容器中。

4. 获取应用配置项，这里的应用是阿里云集群的“应用”在本系统中的一个对应概念。应用配置项包括集群ID（可支持多集群），文件系统磁盘ID、挂载域名、挂载目录等，阿里云应用路由列表，需要挂载的（一个或多个）数据卷名称，镜像地址，分配给该应用的硬件资源，数据卷等。这些配置项遵循阿里云编排模板的格式（支持Docker Compose格式），通过后续的“在阿里云集群创建应用”定义和部署多容器应用。

5. 创建应用启动脚本，该脚本是shell脚本，内容上可以分为main.sh, config.sh, task.sh三部分，其中main.sh由Compose YAML配置文件的entrypoint字段指定，作为容器启动后执行的入口，它首先以root身份调用config.sh，然后以任意Linux用户身份（默认为root）调用task.sh；config.sh中主要是关于深度学习框架环境及相关工具的优化和个性化配置；task.sh首先通知核心应用层，用户作业已启动（通过HTTP API将作业状态从“配置中”修改为“运行中”，然后执行用户所指定的作业启动命令。

6. 创建工作数据卷。这里的工作数据卷指的是，将第三步拷贝用户代码所述可读写文件夹，创建为一个集群数据卷，以挂载到集群中的容器中。这个操作的施行有以下关键保证：用户代码文件，无论是只读的，还是可读写的，均位于NAS文件系统中，云接口所在的机器已通过nfs4协议挂载了该文件系统，容器集群也可以访问该NAS文件系统。

7. 创建数据集数据卷。这里的数据集指的是用户作业所指定的数据集文件。

8. 在阿里云集群创建应用。调用相关接口时，主要参数包括：应用名称；第四步所获取的Compose模版。

   至此，作业已成功“实例化”，并且开始运行。

2）作业生命周期的“追踪”

在1）中已提到修改作业状态，作业状态是作业生命周期的格式化表述。本系统中，作业的状态包括"排队中", "创建中", "配置中", "运行中", "已超时", "已失败", "已结束", "已关闭"，这些状态由核心应用层维护，云接口需要根据作业情况向核心应用层通知作业状态的变更。此外，云接口需要将作业状态的变更情况以日志形式写入到作业的worker日志中。对应于worker日志，作业还有container日志，显然container日志是作业在容器中运行所输出、采集的日志，而worker日志，是作业自提交之初，到正式运行之前，以及运行结束后，有关作业调度情况的信息，这些信息主要由云接口写入。这些日志，将按时间序列整合，以供用户查看。

3）镜像持续集成方案

暂略。

Q&A:
- 为什么基于阿里云容器集群服务？

>底层基础设施不是本系统的核心内容，选用成熟方案，可以快速构建系统。

- 为什么选用Celery？（注：Celery官方只有Python版本）
>这部分代码需求变更频繁，且对性能不敏感，因此选用Python；分布式的设计要求，Celery是Python社区事实上的分布式异步任务标准。

## 4.2 核心调度层
核心调度层负责接收作业请求，调用核心算法模块进行实时定价和调度排期，并对基础设施层发出资源分配和调度指令。核心调度层的设计，主要参考了ERA框架[ref]中的ERA核心。
### 4.2.1 设计目标
#### 4.2.1.1 设计职能目标

如前文所述，核心调度层是ERA框架在本系统中的集成，接受核心应用层的调度请求，通过一系列算法模块，决定具体调度方案，并将调度指令发送给基础设施层的云接口。

#### 4.2.1.2 设计性能目标

理想情况下，核心调度层在本系统中应该是非侵入式的，即是说，核心应用层不必向核心调度层转发用户接入层发出的作业请求，转而直接向基础设施层发出调度指令，通常的，这类调度是遵循FIFO准则的[ref]。但本系统将这一调度过程抽离出来，并且可能要执行一系列复杂的算法，因此，核心调度层应该保持尽可能高性能，尤其是在接口处理方面需要降低开销，从而减轻松耦合带来的负面作用。我们将引入核心调度层的时间开销作为衡量核心调度层的关键性能指标，即核心应用层接受用户接入层的调度请求之后，转发该请求并收到响应的这一时间开销。理论上我们希望这一时长越短越好。

### 4.2.2 设计方案

#### 4.2.2.1 定价和调度算法描述
参考ERA框架中所提出的Basic-Bacon调度算法，本系统采用如下所述的调度算法：
```
//Input: a new job request {W*T in [A, D), V}
//Output: accept or reject, and price if accepted
//procedure MAKE RESERVATION
//    for each t ∈ [A,D) do
//        demand(t) ← the demand estimate function at t
//        for each i ∈ [1, W] do
//            price(t)(i) ← the highest price p s.t.demand(t)(p) + promised[t] + i > Capacity
//        cost[t] ← price(t)(1) + price(t)(2) + ... + price(t)(W)
//    for each t ∈ [A, D-T] do
//        totalCost[t] ← cost[t] + ... + cost[t+T-1]
//    t* ← arg min(t ∈ [A, D-T])totalCost[t]
//    if V >= totalCost[t*] then
//        schedule the job to start at t*
//        return accept at cost totalCost[t*]
//    else
//        return reject
//end procedure
```


本系统对以下资源进行计价：

1. CPU核心数

2. 内存占用大小

3. GPU显卡数

4. 资源占用时长

5. 深度学习框架

   ​

### 4.2.3 实现方案

#### 4.2.3.1 运行环境
操作系统：主流Unix系统，包括Linux/OS X。
软件依赖：Docker, Go, Redis, MySQL。

#### 4.2.3.2 关键实现

为了体现核心调度层的松耦合特性，本系统采用Go语言编写核心调度层（基础设施层的云接口使用Python语言编写），对上层实现了Celery消息协议，对下层（主要指核心应用层）提供了TCP接口处理作业请求。

本系统对该层的实现，表现为"ERA核心"。由Go语言编程实现。
模块结构如下：

- goERACore

  - cloud

    > cloud模块订阅了"accepted_queue"频道；承担云接口的user角色，收到订阅消息后向云接口的broker中发送异步任务消息。由于Celery本身只提供了Python版本，本模块需要实现Celery的消息协议。

  - core

    > 实现了定价和调度算法。接受作业请求后，将调度信息发布到redis中的"accepted_queue"频道。

  - user

    > 提供tcp接口接受作业请求。

架构如下图所示：

![](http://opkk27k9n.bkt.clouddn.com/18-5-1/1418618.jpg)

Q&A:

- 为什么用Go？

  > 对于原有云系统来说，核心调度层应该是非侵入式的、语言无关的，而设计方案中论证了核心调度层应具有较高的响应性能，这就要求算法和网络都要足够快，算法方面，Go语言是编译型语言，相较于脚本语言速度优势明显；网络方面，Go语言对并发的天然支持也有巨大优势。

- 为什么要有"accepted_queue"频道？

  > 解耦。（子模块core和cloud的解耦，事实上，本系统的运行状态下，cloud子模块表现为一个单独的进程，user子模块表现为另一个单独的进程，而core子模块是供user子模块调用的。）


如图所示，核心调度层的运行时至少有两个进程，其中user接口模块对应于TCP API，负责接收TCP请求，解析请求内容，调用core包中的预测、定价、调度算法，将调度排期结果写到redis的accepted_queue频道（这里利用了redis提供的发布订阅机制）并返回给TCP客户端；cloud接口模块对应于Celery Task Submitter，负责从redis中读取调度排期结果，并向云接口的作业调度消息代理发送符合Celery消息协议规范的消息。



## 4.3 核心应用层
核心应用层直接接受客户端的作业请求，并转发给核心调度层，从核心调度层获取响应后再向客户端发出响应，全程对于客户端来说是同步的（非异步），这就要求核心调度层具有非常高的响应速度。
### 4.3.1 设计目标
#### 4.3.1.2 设计职能目标
#### 4.3.1.3 设计性能目标

### 4.3.2 设计方案
针对2.1节中提出的服务内容需求，本系统对其中一些非通用性服务给出如下设计方案：
- 核心应用服务
  采用通用的HTTP服务器应用程序架构，并作为其他服务的管理者履行用户访问权限校验职能。
- 用户文件服务
  采用websocket技术实现文件传输（上传和下载），采用基础设施层的NAS用于文件存储。用户文件服务独立于核心应用服务而存在，可以实现文件模块解耦，提升性能和稳定性。
- 用户作业监控服务
  基于基础设施层的日志采集、性能监控等服务，核心应用层对这些监控数据进行权限管理和websocket传输等封装。此外，深度学习训练过程可视化则主要借由tensorboard实现，支持但不仅限于热门的tensorflow框架。
- 数据可视化服务
  该服务将基于Jupyter Notebook及其扩展，构建中心化的服务器集群，在用户接入层提供数据可视化服务。
此外，对于用户接入层发出的作业请求，核心应用层对该请求内容需要做解析转义，部分规则如下：

[TODO]

### 4.3.3 实现方案
#### 4.3.3.1 运行环境
操作系统：主流Unix系统，包括Linux/OS X。
软件依赖：Docker, Python（包括Flask, Tornado, SQLAlchemy等第三方包）， Redis, MySQL, Circle CI（可选）。
#### 4.3.3.2 关键实现
结合2.1节中需求分析及4.3.2节中的设计方案，本系统对核心应用层各服务的实现方案如下：
① 核心应用服务
基于Flask框架构建的高可扩展性的HTTP应用服务器，并采用uwsgi+supervisor+nginx方案进行部署。数据库方面，选用MySQL并采用了阿里云提供的读写分离架构，且编写了大量API以便查询、更新数据记录。
③ 用户文件服务
该服务基于Tornado构建，并将NAS挂载于Tornado所在的服务器主机上，提供文件上传、下载、浏览、搜索功能。
④ 用户作业监控服务
用户作业的终端日志存储在NAS数据盘的相应文件夹下，性能监控数据存储在独立的influxdb时序数据库中，通过Tornado搭建http/websocket API，供用户接入层访问这些数据。
⑤ 镜像定制发布服务
镜像定制方面，建立镜像文件（Dockerfile）代码仓库，采用Circle CI进行持续集成，主要流程如下：检测镜像文件更新，执行镜像构建，将镜像推送到镜像仓库。
镜像发布方面，所有镜像的信息封装在一个hash表，存储在Redis中。开发者通过写该数据结构完成镜像发布。

## 4.4 用户接入层
用户接入层面向用户提供系统接入，收集用户输入构造HTTP请求，发送给核心应用层，获取响应并解析给用户查看。
### 4.3.1 设计目标
#### 4.3.1.1 设计职能目标

用户接入层面向用户提供系统接入，收集用户输入构造HTTP请求，发送给核心应用层，获取响应并解析给用户查看。

#### 4.3.1.2 设计性能目标

以用户提交作业请求的提交动作为起始时刻，到最终的实际HTTP请求发出，这之间在用户接入层的处理时间，称为”客户端处理耗时“。我们希望这一耗时越短越好。

#### 4.3.1.3 实现方案

#####4.3.1.3.1 系统环境和软件依赖

全面支持主流桌面操作系统命令行，包括Windows的cmd，Linux和OS X等类Unix系统的terminal；
软件依赖：

- Python 2.7/3.6
- [pip（可选）](https://pypi.org/project/pip/)
- [click](http://click.pocoo.org/5/)

#### 4.3.1.3.1 关键实现

本系统对该层的实现，表现为"客户端CLI"。使用Python实现，Python2/Python3兼容，可以使用pip/anaconda等包管理工具安装。CLI支持如下参数：

- '--data', 字符串类型，支持多参数，需要挂载的数据集ID

- '--jupyter/--no-jupyter', 布尔类型，是否打开jupyter模式，默认为false不打开

- '--resubmit/--no-resubmit',布尔类型，是否对上次提交作业重新竞价，默认false

- '--eager/--no-eager', 布尔类型，是否需要尽可能快地立即执行作业，默认false

- '--value',浮点类型，竞价金额，单位“元”

- '--duration', 字符串类型，预期作业运行时长

- '--earliest',字符串类型，预定的时间窗口的开始时刻

- '--deadline',字符串类型，预定的时间窗口的结束时刻

- '--tensorboard/--no-tensorboard', 布尔类型，是否打开tensorboard服务，默认为false不打开

- '--env',字符串类型，需要使用的深度学习框架环境的名称

- '--os',字符串类型，需要使用的操作系统

- '-gt', '--gputype',字符串类型，需要使用的GPU显卡类型

- '-gn', '--gpunum',整数类型，需要使用的GPU显卡数量

- '-cn', '—cpunum', 整数类型，需要使用的CPU核心数

- '-mn', '--memnum',浮点类型，需要使用的内存大小，单位GiB

  ​

## 4.5 本章小结
本章介绍了本文所述系统的详细设计方案及其关键实现论述，本系统包括基础设施层、核心调度层、核心应用层和用户接入层，每一层各司其职，其中基础设施层和核心调度层构成了本文的核心研究内容。

下一章将对本章所设计实现的系统进行实际运行测试，并就测试结果进行分析，检验本系统的各项功能以及性能指标。

# 5. 测试与分析
## 5.1 功能测试
### 5.1.1 基础设施层
### 5.1.2 核心调度层
### 5.1.3 核心应用层
### 5.1.4 用户接入层
### 5.1.4 系统整体功能测试

## 5.2 性能测试
### 5.1.1 基础设施层
### 5.1.2 核心调度层
### 5.1.3 核心应用层
### 5.1.4 用户接入层
### 5.1.5 系统整体性能测试

## 5.3 本章小结

本章通过将系统各组件部署到真实的实验环境，执行各项测试，分别从功能以及性能两个角度考察本系统，检验了本文所述系统设计方案及其实现是行之有效的：一方面，本系统简化缩短了深度学习研发者用户开启深度学习的繁琐流程，一定程度上提高了开发效率；另一方面，本系统通过预定以及共享计算能力的方式，有效地提高了云资源利用率。

# 6. 总结
## 6.1 全文总结

本文从深度学习开发者用户所面临的实际问题出发，指出了现有云端系统在深度学习领域的落地存在后天不足，而现有深度学习平台在资源分配方面存在资源闲置、利用率不足的问题。因此，本文提出以面向深度学习领域提供平台级服务为目标，为深度学习开发者用户提供更为易用的服务。此外，本文提出将深度学习云端系统的核心问题定义为任务调度，结合成熟的分布式任务消息队列技术，以及Docker容器集群和云存储等云计算技术，设计并实现了面向深度学习的一种基于ERA的任务调度系统。本文第四章详细阐述了该系统的设计方案、实现方案以及运行环境，在第二章则对设计与实现方案的关键支撑技术做了详细介绍。本文第五章则介绍了本系统的运行情况，证明了该解决方案是行之有效的。

## 6.2 课题展望

本文所述系统，其中部分设计与实现方案已在国内的RussellCloud深度学习云计算平台得以应用，通过真实的工业界实践证明了其有效性。本文的一个贡献是，对在真实云系统中集成ERA框架做了有益尝试。但是，ERA框架的集成仍然是有代价的，对于真实的云系统来说，ERA框架对系统输入提出了较为严格且细致的要求，且一定程度上牺牲了作业调度的灵活性，因此本文4.2所述的核心调度层，并未在真实用户环境下得以实践测试。

可以展望的是，ERA框架所提出的预定准则，可以给系统学领域带来更多经济学的思考。同时，深度学习的蓬勃发展，也将促进资源调度需求预测算法的持续进步，从而实现更有效的资源分配。此外，对于面向开发者用户甚至普通用户的云端系统而言，用户体验仍然是需要关注的领域，我们需要权衡用户体验与系统效率之间的得失。

# 7. 致谢

恍然间，大学四年匆匆而过，我已临近毕业。从进校军训开始，到离校毕设结束，这期间充满了太多言语所不能及。

首先，感谢我的课题指导老师钟国辉老师，他在把关课题方向和论文质量方面给予了我充分的指导和帮助，无论是学识上还是做人做事方面我都学到许多。其次，我要感谢Dian团队的研究生师兄和种子班的同学们，朋辈的帮助与交流让我对课题有了更多信心，也萌生了许多新鲜想法得以实践。同时，还要感谢RussellCloud项目团队的所有人，我们一起通力协作，让RussellCloud成为了更好的深度学习云计算平台，这也让本课题产生了实际意义。最后，感谢华科，感谢这里的所有人和所有地方，给我们创造了一个更好的学习和研究环境。



# 附录

### 引用

- 虚拟化技术概述：http://dockone.io/article/1562
- Kubernetes介绍：https://kubernetes.io/
- Redis介绍：https://redis.io/, https://github.com/antirez/redis
- 自己动手写Docker 序 阿里云容器服务团队架构师易立
- Distributed task scheduler for computing environments：https://patents.google.com/patent/US20050268300A1/en
- Celery Protocol: http://docs.celeryproject.org/en/latest/internals/protocol.html