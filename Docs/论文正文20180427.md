题目：面向深度学习的一种基于ERA的任务调度系统的设计与实现

# 摘要
# 1. 绪论
## 1.1 研究背景及意义
近年以来，随着计算机单机计算及分布式计算能力的大幅提升，以及神经网络算法方面的持续深入发展，深度学习开始更广泛地应用于人工智能各个领域，促进了计算机视觉、数据挖掘、语音识别、自然语言处理等交叉学科的迅猛发展，成为近些年来人工智能领域最受瞩目的领域。不仅高校、企业通过校企合作、成立研究院、实验室等广泛开展深度学习研究，由于一些深度学习框架的开源和数据开放，传统信息技术领域也涌现了许多深度学习爱好者。但是，搭建深度学习平台的成本相当高昂，流程相对复杂，具有一定的工程技术要求。硬件方面，由于深度学习研究对计算能力的高要求，需要购买高性能GPU、CPU、SSD、主机等；软件方面，通常需要搭建多种深度学习框架，并构建作业提交机制。此外，由于诸如计算机CPU空转等众所周知的因素，目前已部署的深度学习平台资源利用率较低[1]。因此，如何面向深度学习领域简化其研究和开发流程，规范化、自动化深度学习研究领域的运营和维护工作，并有效提高资源利用率以降低经济成本，将成为深度学习研发过程中面临的一个重要问题。
深度学习平台的云端化，已成为业内共识和普遍趋势。所谓云端化，依托于云计算的快速发展。云计算，是指计算资源（如CPU，GPU，存储等）作为可以出租的通用工具，按需提供给用户租用。云计算的出现对整个信息技术行业产生了巨大的影响，谷歌、亚马逊、微软、阿里巴巴等大公司试图成为类似电信运营商的云计算服务提供商，提供更强大、更可靠、更低成本的云计算平台，并且已在互联网领域取得了相当多的最佳实践。
云计算服务通常意义上分为三类：基础设施即服务（IaaS，infrastructure as a service），为用户提供硬件设施（服务器、存储、网络）和相关软件（虚拟化操作系统、文件系统）服务；平台即服务（PaaS, platform as a service），通过Web向开发人员提供应用程序开发和部署平台服务；软件即服务（SaaS, software as a service），为用户提供托管的软件（运行在平台和基础设施上）。

## 1.2 国内外研究现状
随着虚拟化技术的发展和云计算技术的日益成熟，越来越多的企业专注于为企业用户乃至个人用户提供PaaS服务。一些 PaaS 供应商使用容器来减少为每个应用程序创建一个新的虚拟机的开销, 从而降低了运行 PaaS 应用程序的成本, 同时在进程、网络和文件系统级别保持隔离，保证了可移植性和安全性。相比于IaaS，PaaS专注于垂直领域，更趋向定制化。而深度学习的发展历程较短，发展速度较快，可谓日新月异，因此还未出现一家成熟的PaaS服务商，为深度学习研究者提供应用级服务。深度学习研究者只能使用IaaS服务商提供的云计算资源，按图索骥一步步搭建定制化的深度学习环境。这一过程中，开发者不得不投入时间和精力来管理计算资源、搭建相应软件环境、部署应用程序。随着深度学习研究与应用的普及，部分云计算企业了发布深度学习领域的PaaS平台，为深度学习研究者提供更精细化的服务。目前出现的深度学习PaaS平台主要有以下三类： 
（1）提供训练环境以及周边功能，能够将深度学习模型快速转化为应用。
（2）仅提供深度学习云计算服务，提供了基本的快速部署和训练深度学习模型训练功能。
（3）提供定制化的深度学习环境，提供某种特定深度学习框架的软件环境。
	以上现有的平台并不能解决目前深度学习研发所面临的问题，包括FlyodHub、TinyMind、RussellCloud等在内的该领域初创企业的产品，在可用性、易用度和性价比（由云资源利用率反映）方面仍然有很大的优化空间。而Azure和Bluemix提供的深度学习平台的关键弱点是可扩展性，与开源深度学习框架的集成度较低，且在云资源利用率方面乏善可陈。


## 1.3 论文结构
本文全文结构如下：
第一章，绪论，主要介绍了本文所涉及研究领域的现状及动态，包括云计算的各类形式和层次、工业届的深度学习作业工作流以及任务调度方案、目前所面临的问题等，在本章最后提出了解决方案：“面向深度学习的一种基于ERA的任务调度系统”，并对其作了简要描述。

第二章，相关技术介绍。介绍了与本文所设计并实现的系统相关度较高的几项关键技术。

第三章，需求分析。从任务调度与资源分配这一系统学领域，以及深度学习这一垂直细分的业务领域，对本系统旨在满足的需求进行了完整分析，且提出了检验本系统的一些标准。

第四章，设计与实现。本章首先提出了一个层次分明的总体架构，接下来进行分层设计，对各层的功能和性能提出设计要求，然后再提出设计方案，并根据该设计方案给出具体实现方案。

第五章，测试与分析。本章记录了第四章中所论述实现的系统的实际运行测试结果，包括功能和性能两方面的，并结合第二章所提出的检验标准进行针对性分析。

第六章，总结与展望。本章给出现有阶段下本系统的不足，对未来的进一步优化进行展望。

# 2. 相关技术介绍

## 2.1 docker相关技术介绍
### 2.1.1 虚拟化技术概览
虚拟化（Virtualization）技术最早出现在 20 世纪 60 年代的 IBM 大型机系统，在70年代的 System 370 系列中逐渐流行起来，这些机器通过一种叫虚拟机监控器（Virtual Machine Monitor，VMM）的程序在物理硬件之上生成许多可以运行独立操作系统软件的虚拟机（Virtual Machine）实例。随着近年多核系统、集群、网格甚至云计算的广泛部署，虚拟化技术在商业应用上的优势日益体现，不仅降低了 IT 成本，而且还增强了系统安全性和可靠性，虚拟化的概念也逐渐深入到人们日常的工作与生活中。

虚拟化技术主要分为以下几个大类 [1]：
- 平台虚拟化（Platform Virtualization），针对计算机和操作系统的虚拟化。
- 资源虚拟化（Resource Virtualization），针对特定的系统资源的虚拟化，比如内存、存储、网络资源等。
- 应用程序虚拟化（Application Virtualization），包括仿真、模拟、解释技术等。
我们通常所说的虚拟化主要是指平台虚拟化技术，通过使用控制程序（Control Program，也被称为 Virtual Machine Monitor 或 Hypervisor），隐藏特定计算平台的实际物理特性，为用户提供抽象的、统一的、模拟的计算环境（称为虚拟机）。虚拟机中运行的操作系统被称为客户机操作系统（Guest OS），运行虚拟机监控器的操作系统被称为主机操作系统（Host OS），当然某些虚拟机监控器可以脱离操作系统直接运行在硬件之上（如 VMWARE 的 ESX 产品）。运行虚拟机的真实系统我们称之为主机系统。

![](http://opkk27k9n.bkt.clouddn.com/18-4-27/93911199.jpg)

实现 IT 基础架构的虚拟化可以带来以下好处：
1. 效率：将原本一台服务器的资源分配给了数台虚拟化的服务器，有效的利用了闲置资源，确保企业应用程序发挥出最高的可用性和性能。
2. 隔离：虽然虚拟机可以共享一台计算机的物理资源，但它们彼此之间仍然是完全隔离的，就像它们是不同的物理计算机一样。因此，在可用性和安全性方面，虚拟环境中运行的应用程序之所以远优于在传统的非虚拟化系统中运行的应用程序，隔离就是一个重要的原因。
3. 可靠：虚拟服务器是独立于硬件进行工作的，通过改进灾难恢复解决方案提高了业务连续性，当一台服务器出现故障时可在最短时间内恢复且不影响整个集群的运作，在整个数据中心实现高可用性。
4. 成本：降低了部署成本，只需要更少的服务器就可以实现需要更多服务器才能做到的事情，也间接降低了安全等其他方面的成本。
5. 兼容：所有的虚拟服务器都与正常的x86系统相兼容，他改进了桌面管理的方式，可部署多套不同的系统，将因兼容性造成问题的可能性降至最低。
6. 便于管理：提高了服务器/管理员比率，一个管理员可以轻松的管理比以前更多的服务器而不会造成更大的负担。

### 2.1.2 Docker简介
Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护（项目地址：https://github.com/moby/moby）。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。

Docker 自开源后受到广泛的关注和讨论，至今其 GitHub 项目已经超过48000个star和14000个fork。甚至由于 Docker 项目的火爆，在 2013 年底，dotCloud 公司决定改名为 Docker。Docker 最初是在 Ubuntu 12.04 上开发实现的；Red Hat 则从 RHEL 6.5 开始对 Docker 进行支持；Google 也在其 PaaS 产品中广泛应用 Docker。

Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。

Docker 在容器的基础上，进行了进一步的封装，从文件系统、网络互联到进程隔离等等，极大的简化了容器的创建和维护。使得 Docker 技术比虚拟机技术更为轻便、快捷。

传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。

Docker与传统虚拟化技术的比较：
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/77539209.jpg)
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/92916237.jpg)

Docker公司的创始人兼CTO——Solomon Hykes，有机地把一系列技术Cgroups、Namespace和UnionFS整合起来，极大地降低了容器技术的复杂度，提升了开发者的用户体验。他敏锐地预测到，一旦标准化容器技术最终出现，整个技术行业将会受到深远的影响。Docker公司开源了Docker Engine，定义了一个以容器镜像为标准的应用打包格式，并且简历Docker Hub服务（https://hub.docker.com）进行镜像分发和协作。这些举措迅速创建了一个良好的社区和合作伙伴生态圈，包含AWS、Google、Microsoft、IBM和国内的众多公司。在短短几年的时间内，Docker几乎成为了容器技术的代名词。

### 2.1.3 基本概念：镜像、容器和仓库
docker 包括三个基本概念：
- 镜像（Image）
- 容器（Container）
- 仓库（Repository）
#### 2.1.3.1 Docker 镜像
我们都知道，操作系统分为内核和用户空间。对于 Linux 而言，内核启动后，会挂载 root 文件系统为其提供用户空间支持。而 Docker 镜像（Image），就相当于是一个 root 文件系统。
Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。

Docker镜像具有分层存储的特性。因为镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。所以严格来说，镜像并非是像一个 ISO 那样的打包文件，镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。

镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。

分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。

#### 2.1.3.2 Docker容器
镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。

容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。

前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。

容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。

按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录，在这些位置的读写会跳过容器存储层，直接对宿主（或网络存储）发生读写，其性能和稳定性更高。

数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。
#### 2.1.3.3 Docker Registry
镜像构建完成后，可以很容易的在当前宿主机上运行，但是，如果需要在其它服务器上使用这个镜像，我们就需要一个集中的存储、分发镜像的服务，Docker Registry 就是这样的服务。

一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。

通常，一个仓库会包含同一个软件不同版本的镜像，而标签就常用于对应该软件的各个版本。我们可以通过 <仓库名>:<标签> 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。

以 Ubuntu 镜像 为例，ubuntu 是仓库的名字，其内包含有不同的版本标签，如，14.04, 16.04。我们可以通过 ubuntu:14.04，或者 ubuntu:16.04 来具体指定所需哪个版本的镜像。如果忽略了标签，比如 ubuntu，那将视为 ubuntu:latest。

仓库名经常以 两段式路径 形式出现，比如 jwilder/nginx-proxy，前者往往意味着 Docker Registry 多用户环境下的用户名，后者则往往是对应的软件名。但这并非绝对，取决于所使用的具体 Docker Registry 的软件或服务。

Docker Registry 公开服务：
Docker Registry 公开服务是开放给用户使用、允许用户管理镜像的 Registry 服务。一般这类公开服务允许用户免费上传、下载公开的镜像，并可能提供收费服务供用户管理私有镜像。

最常使用的 Registry 公开服务是官方的[Docker Hub](https://hub.docker.com/)，这也是默认的 Registry，并拥有大量的高质量的官方镜像。

私有 Docker Registry：
除了使用公开服务外，用户还可以在本地搭建私有 Docker Registry。Docker 官方提供了 Docker Registry 镜像，可以直接使用做为私有 Registry 服务。

### 2.1.4 Docker Compose
Compose是定义和运行多容器Docker应用程序的工具。 使用Compose，我们可以使用YAML文件来配置应用程序的服务。然后，使用单个命令，创建并启动配置中的所有服务。

使用Compose基本上是一个三步过程：
1. 使用Dockerfile定义应用程序的环境，以便在任何地方进行复制。
2. 在docker-compose.yml中定义组成应用程序的服务，以便它们可以在隔离的环境中一起运行。
3. 运行docker-compose并撰写开始并运行整个应用程序。

Compose有以下特性：
- 单个主机上有多个独立的环境
- 创建容器时保留卷数据
- 只重新创建已更改的容器：Compose缓存用于创建容器的配置。重新启动未更改的服务时，Compose将重新使用现有的容器，这意味着我们可以快速更改环境。
- 支持Compose文件中的变量：可以使用这些变量为不同的环境或不同的用户自定义组合。

### 2.1.5 容器集群管理系统介绍
#### 2.1.5.1 Docker Swarm
在Docker 1.12 之前，独立版的Docker Swarm作为Docker的原生集群系统，使用API​​代理系统将Docker主机池变成单个虚拟主机。它是2014年开始的Docker第一个容器编排项目。结合Docker Compose，这是一个用来安排容器的非常方便的工具。其灵活性和简单性使其易于与现有IT基础架构集成。许多公司和用户已经将Docker Swarm独立版部署用于生产和实验项目。 Docker目前没有计划反对Docker Swarm。 Docker API向后兼容，因此Docker Swarm将继续与未来的Docker Engine版本一起工作。

在Docker 1.12及更高版本中，Swarm模式已经集成到Docker Engine中。

Swarmkit：Docker Engine 1.12或更高版本中的集群管理和编排功能。当启用Swarmkit时，我们调用Docker Engine以Swarm模式运行。

Swarm模式内置 kv 存储功能，提供了众多的新特性，比如：具有容错能力的去中心化设计、内置服务发现、负载均衡、路由网格、动态伸缩、滚动更新、安全传输等。使得 Docker 原生的 Swarm 集群具备与 Mesos、Kubernetes 竞争的实力。

swarm mode特性介绍：
1. 与Docker Engine集成的集群管理：使用Docker Engine CLI创建一群Docker引擎，您可以在其中部署应用程序服务。 您不需要额外的编排软件来创建或管理群。
2. 声明式服务模型：Docker Engine使用声明式方法让您在应用程序堆栈中定义各种服务的所需状态。例如，您可能会描述一个由带有消息队列服务和数据库后端的Web前端服务组成的应用程序。
3. 缩放：对于每个服务，您可以声明要运行的任务数量。当您向上或向下缩放时，s​​warm管理器会通过添加或删除任务来自动调整以保持所需的状态。
4. 期望的状态协调：swarm manager节点持续监视集群状态，并协调实际状态与表达期望状态之间的任何差异。例如，如果您设置了一个服务来运行一个容器的10个副本以及一个承载其中两个副本崩溃的工作机，则该管理器会创建两个新副本来替换发生崩溃的副本。 swarm manager将新副本分配给正在运行且可用的工作人员。
5. 多主机联网：您可以为您的服务指定覆盖网络。 swarm管理器在初始化或更新应用程序时自动为覆盖网络上的容器分配地址。
6. 服务发现：Swarm管理器节点为swarm中的每个服务分配一个唯一的DNS名称并负载平衡正在运行的容器。您可以通过群集中嵌入的DNS服务器查询群集中运行的每个容器。
7. 负载平衡：您可以将服务的端口暴露给外部负载平衡器。在群集内部，您可以指定如何在节点之间分发服务容器。
8. 默认情况下为安全：群中的每个节点都强制进行TLS相互认证和加密，以保护其自身与所有其他节点之间的通信。您可以选择使用自定义根证书或来自自定义根CA的证书。
9. 滚动更新：在推出时，您可以逐步将服务更新应用于节点。 swarm管理器允许您控制服务部署到不同节点集之间的延迟。如果出现任何问题，您可以将任务回滚到以前版本的服务。

#### 2.1.5.2 Kubernetes
Kubernetes 是 Google 团队发起并维护的基于 Docker 的开源容器集群管理系统，它不仅支持常见的云平台，而且支持内部数据中心。

建于 Docker 之上的 Kubernetes 可以构建一个容器的调度服务，其目的是让用户透过 Kubernetes 集群来进行云端容器集群的管理，而无需用户进行复杂的设置工作。系统会自动选取合适的工作节点来执行具体的容器集群调度处理工作。其核心概念是 Container Pod。一个 Pod 由一组工作于同一物理工作节点的容器构成。这些组容器拥有相同的网络命名空间、IP以及存储配额，也可以根据实际情况对每一个 Pod 进行端口映射。此外，Kubernetes 工作节点会由主系统进行管理，节点包含了能够运行 Docker 容器所用到的服务。

Kubernetes具有以下特性：
1. 自动装箱：根据资源需求和其他限制自动放置容器，同时不牺牲可用性。混淆重要和尽力服务的工作负载，以提高利用率并节省更多资源。
2. 水平缩放：使用一个简单的命令，使用UI或根据CPU使用情况自动扩展和缩小您的应用程序。
3. 自动发布和回滚：Kubernetes逐步推出对应用程序或其配置的更改，同时监视应用程序运行状况以确保它不会同时终止所有实例。 如果出现问题，Kubernetes会为您回滚更改。 充分利用日益增长的部署解决方案生态系统。
4. 存储编排：自动安装您选择的存储系统，无论是从本地存储，公共云提供商（如GCP或AWS）还是网络存储系统（如NFS，iSCSI，Gluster，Ceph，Cinder或Flocker）。
5. 自愈：重新启动失败的容器，在节点死亡时替换容器并重新安排容器，杀死对用户定义的运行状况检查无响应的容器，并且在准备好提供服务之前不会将它们通告给客户端。
6. 服务发现和负载均衡：无需修改应用程序即可使用不熟悉的服务发现机制。 Kubernetes为容器提供了自己的IP地址和一组容器的单个DNS名称，并且可以在它们之间进行负载平衡。
7. 秘密和配置管理：部署和更新秘密和应用程序配置，无需重新构建映像，也不会在您的堆栈配置中暴露秘密。
8. 批量执行：除了服务之外，Kubernetes还可以管理您的批处理和CI工作负载，并根据需要替换失败的容器。

![](http://opkk27k9n.bkt.clouddn.com/18-4-27/19557914.jpg)

## 2.2 分布式异步任务调度技术介绍
任务调度是计算环境中的一个重要特性。在本文所述的系统中，要执行的任务主要是深度学习计算任务。

当前的单机任务系统，意味着任务列表（或调度数据）是本地存储的，并位于执行任务的同一台计算机上。虽然这些系统在单个计算机环境中运行良好，但是可扩展能力有重要缺陷。例如，如果计划在同一时间或几乎同时运行多个任务，单主机可能会很快变得负担过重或资源不足，结果导致某些任务可能无法正确执行。因此，分布式的任务调度已经成为云计算领域的关键技术之一，我们需要一套将任务管理与任务执行分开的分布式任务调度方法和系统：其中任务管理和任务运行在独立的计算设备上执行，而任务管理由至少一个数据中介执行，而任务执行由多个执行主机执行。对于此类系统，消息的分布式传递是其核心内容之一。

### 2.1 Celery简介
Celery是基于分布式消息传递的异步任务队列/作业队列。 它专注于实时操作，但也支持调度。执行单元（称为任务）在使用多处理，Eventlet或gevent的单个或多个工作服务器上同时执行。 任务可以异步执行（在后台）或同步执行（等待直到准备就绪）。

Celery具有以下特性：
1. 易于集成：Celery很容易与Web框架集成，其中一些甚至包含集成包。Celery是用Python编写的，但协议可以用任何语言实现。 它也可以使用webhooks与其他语言一起运行。
2. 支持多消息代理
推荐的消息代理是RabbitMQ，但也支持Redis，Beanstalk，MongoDB，CouchDB和数据库（使用SQLAlchemy或Django ORM）。

### 2.2 Celery的消息协议
#### 2.2.1 第二版定义
```python
properties = {
    'correlation_id': uuid task_id,
    'content_type': string mimetype,
    'content_encoding': string encoding,

    # optional
    'reply_to': string queue_or_url,
}
headers = {
    'lang': string 'py'
    'task': string task,
    'id': uuid task_id,
    'root_id': uuid root_id,
    'parent_id': uuid parent_id,
    'group': uuid group_id,

    # optional
    'meth': string method_name,
    'shadow': string alias_name,
    'eta': iso8601 ETA,
    'expires': iso8601 expires,
    'retries': int retries,
    'timelimit': (soft, hard),
    'argsrepr': str repr(args),
    'kwargsrepr': str repr(kwargs),
    'origin': str nodename,
}

body = (
    object[] args,
    Mapping kwargs,
    Mapping embed {
        'callbacks': Signature[] callbacks,
        'errbacks': Signature[] errbacks,
        'chain': Signature[] chain,
        'chord': Signature chord_callback,
    }
)
```

### 2.2.2 任务序列化
任务消息头部的content_type字段可以指定一些序列化格式，默认支持的MIME类型见下表：

|Scheme	|MIME| Type|
|---|---|---|
|json	|application/json|
|yaml	|application/x-yaml|
|pickle	|application/x-python-serialize|
|msgpack|application/x-msgpack|

## 2.3 数据库介绍
### 2.3.1 Redis
Redis是一个开源的（BSD许可）内存数据结构存储，可用作数据库、缓存和消息代理。它支持的数据结构，如字符串，散列，列表，集合，具有范围查询的排序集，位图，超级日志和具有半径查询的地理空间索引。 Redis具有内置复制，Lua脚本，LRU驱逐，事务和不同级别的磁盘持久性，并通过Redis Sentinel提供高可用性，并通过Redis集群实现自动分区。[ref]

Redis经常被称为数据结构服务器。这意味着Redis通过一组命令提供对可变数据结构的访问，这些命令是使用带有TCP套接字和简单协议的服务器 - 客户机模型发送的。因此，不同的进程可以以共享的方式查询和修改相同的数据结构。

Redis中实现的数据结构有一些特殊的属性：
- Redis会将它们存储在磁盘上，即使它们始终被提供并修改到服务器内存中。这意味着Redis速度很快，但也是非易失性的。
- 数据结构的实现强调内存效率，因此与使用高级编程语言建模的相同数据结构相比，Redis内部的数据结构可能会使用更少的内存。
- Redis提供了很多在数据库中很自然的功能，如复制，可调节级别的耐用性，集群，高可用性。

另一个很好的例子是将Redis看作memcached的更复杂版本，其中操作不仅仅是SET和GET，而是用于处理像列表，集合，有序数据结构等复杂数据类型的操作。

### 2.3.2 MySQL
MySQL 是一种快速易用的 RDBMS，很多企业（不分规模大小）都在使用它来构建自己的数据库。MySQL 由一家瑞典公司 MySQL AB 开发、运营并予以支持。MySQL是一种关系型数据库管理系统（RDBMS），支持标准的结构化查询语言（Structured Query Language）。

MySQL具有以下特性：
1. 使用C和C++编写，保证了源代码的可移植性。
2. 支持多种操作系统。
3. 为多种編程语言提供了API。
4. 支持多线程，充分利用CPU资源，支持多用户。
5. 優化的SQL查询算法，有效地提高查询速度。
6. 既能够作为一个单独的应用程序在客户端服务器网络环境中运行，也能够作为一个程序库而嵌入到其他的软件中。
7. 对多种字符集的完全支持。
8. 提供TCP/IP、ODBC和JDBC等多种数据库连接途径。
9. 提供用于管理、检查、優化数据库操作的管理工具。
10. 可以处理拥有上千万条记录的大型数据库。

## 2.4 本章小结
总览本章，2.1节着重介绍了docker相关的容器化技术栈，docker技术栈是本文所论述的任务调度系统所依赖的底层核心组件；2.2节介绍了Celery，它在本系统中负责任务调度消息的分布式分发；2.3节介绍了Redis和MySQL这两种数据库。

# 3. 需求分析

## 3.1 任务调度与资源分配
[ref https://en.wikipedia.org/wiki/Scheduling_(computing)]

在计算中，调度是通过某些方法指定的工作分配给完成工作的资源的方法。这项工作可能是虚拟计算元素，例如线程，进程或数据流，这些虚拟计算元素又被调度到诸如处理器，网络链接或扩展卡之类的硬件资源上。

调度程序是执行调度活动的内容。调度程序经常需要保持所有计算机资源的繁忙（如负载均衡），允许多个用户有效地共享系统资源，或者实现目标服务质量。


调度器可以聚焦于一个或多个目标，例如：最大化吞吐量（每个时间单元完成的工作总量）;最大限度地减少等待时间（从工作开始到第一个点开始执行资源的时间）;最大限度地减少等待时间或响应时间（从批处理活动开始直到完成工作为止的时间[1] [2] [3]，或者直到系统响应并在交互活动时将第一个输出交给用户）; [4]或最大化公平性（每个流程的CPU时间相同，或者根据每个流程的优先级和工作负载更一般地适当的时间）。实际上，这些目标经常发生冲突（例如吞吐量与延迟），因此调度程序将执行适当的折衷。根据用户的需求和目标，优先考虑上述任何一个问题。

在本系统中，任务调度的对象是深度学习作业，任务调度的目标是容器集群。

在容器集群的实现中，事实上已经涉及到资源分配的部分。在本系统中，由于集成了预定这一经济学准则，还需要对资源进行进一步的建模，以更好地结合集群管理器实现预期的资源分配，从而优化任务调度的价值。

## 3.2 深度学习业务需求分析
在一个典型的深度学习环境搭建过程中，用户一般需要完成以下三个环节：
(1)	购买或者租用相应的硬件设施，包括服务器、数据存储器、GPU等设备。
(2)	配置、组装上述设备，使其构成完整的计算工作流，包括操作系统的安装、网络管理和数据盘的挂载等。
(3)	搭建选定的深度学习软件环境，安装相应的软件包、深度学习框架以及必要的通用开发工具等。

完成环境搭建后，用户在搭建好的环境中开始执行深度学习作业之前，由于深度学习作业通常需要大型数据集作为训练数据，因此需要事先下载这些数据集。完成下载后，用户手动启动作业的执行，在这一过程中用户通常会查看作业日志（一般是控制台标准输出，或者是可视化处理过的），以及作业运行的资源占用情况。深度学习作业执行结束后，可能会生成一些模型文件或其他数据文件，用户需要保存这些“任务输出”。

结合如上所述完整的深度学习作业流程，在面向深度学习领域的任务调度系统中，我们需要：
1. 以GPU为代表的硬件支持；
2. 以数据集为代表的海量数据存储支持，以及公有数据集和私有数据集的权限分离；
3. 深度学习软件环境的多样化支持和统一配置；
4. 深度学习作业执行过程的日志监控和性能监控；
5. 保存作业的输出结果。

就具体服务内容而言，用户需要以下服务内容：
- 核心应用服务
主要提供用户接入，包括登入登出、提交作业、查看作业等。此外，该服务为系统管理员提供系统配置接口。
- 用户文件服务
在本文所述深度学习场景下，用户文件主要包括代码文件以及数据文件。其中，代码文件较小，而数据文件则相对较大。用户为执行一项作业，需要上传这项作业中需要使用的这些文件。
- 用户作业监控服务
完成作业运行时关键指标监控、日志监控、深度学习训练过程可视化等，并提供相关访问接口。关键指标包括CPU占用率、内存占用率、磁盘I/O、网络I/O等。此外，深度学习训练过程可视化也是需求之一。
- 镜像定制发布服务
该服务主要针对本平台的运维管理人员，针对当下深度学习框架层出不穷、版本更新快的特点，实现深度学习框架的镜像化定制与发布，以满足用户进行深度学习研究的多样化需求。当然，用户也可以借由该服务实现自定义镜像，实现高扩展性。（这里的镜像指docker技术所使用的image）
- 数据可视化服务
不同于训练过程可视化，本平台提供的数据可视化服务主要针对用户在本平台存储的数据集和作业输出数据。该服务将基于Jupyter Notebook及其扩展，构建中心化的服务器集群以提供数据可视化服务。

## 3.3 检验标准
针对3.1节和3.2节所述需求，本文所设计的系统需要满足以下要求：
- 通过功能测试，准确执行作业（包括起始时间、硬件资源分配、其他作业描述信息等）；
- 根据云资源利用率计算公式，接入核心调度层之后云资源利用率有提高；
（公式： 静态集群的资源利用率=Sum(集群中某机器的CPU占用率在一段时间内的均值 * cpu_ratio + GPU占用率 * gpu_ratio + 内存占用率 * mem_ratio)/某集群机器总数，其中各项ratio之和为1。对于动态伸缩的集群，该公式在计算的时间粒度上作相应调整即可。）

## 3.4 本章小结
本章从任务调度系统本身，以及本系统所在的垂直细分领域——深度学习这一角度，阐述了本系统所需要满足的一些基本需求，并对这些需求的满足提出了检验标准。

本文将在下一章就如何满足本章提出的需求，提出系统的设计方案。

# 4. 设计与实现
TODO
该架构应该是分布式的（每一层的部署都可以是分布式的，层与层也是分布式的）。有一些配置需要共享。

## 4.1 基础设施层
本系统中，基础设施层是对云计算服务商提供的公有云服务的一种概念封装，由于这些云服务是关键性的底层依赖，对其进行的封装设计可以使得整个系统架构设计具有高内聚低耦合的特性。基础设施层的设计，采用计算与存储隔离（灵活配置）、计算与计算隔离（安全可靠）、存储与存储隔离（权限可分）的基本思路。
### 4.1.1 设计目标
#### 4.1.1.1 系统环境和软件依赖
操作系统：现代主流Linux发行版。
### 4.1.1.2 设计职能目标
### 4.1.1.3 设计性能目标

### 4.1.2 设计方案
基础设置层对关键技术的选型如下：
1. 计算方面，选用Docker Engine内置的swarm mode集群。Swarm mode 集群的架构图如下所示:
![](http://opkk27k9n.bkt.clouddn.com/18-4-27/11857606.jpg)
2. 存储方面，选用NAS存储。NAS可以方便地挂载于云服务器、Docker容器，且支持权限组，可以实现文件系统级别的读写权限分离。
3. 其他方面，在计算集群所在局域内网搭建私有Docker Registry，以加速镜像拉取。

理论上，基础设施层也是可以运行于私有云之上的。这源于下述基础设施管理接口的封装设计（为论述方便，将这部分封装设计称为“云接口”）：
1. 从“已排期作业队列”中读取作业标识符（jobId）；
2. 根据jobId，从数据库读取相关作业描述信息（jobDesc）；
3. 根据jobDesc，执行作业。具体执行过程包括（下述过程均封装为可调用接口；在本系统中，“作业进程”是一个或多个具体的容器）：
    - 申请硬件资源；
    - 启动作业进程；
    - 监控作业进程；
    - 回收作业进程。
4. 上述执行过程，均通过调用集群管理器接口实现（将jobDesc集成到调用参数中，通常以yaml的格式）。集群管理器接口包括以下关键内容：
    - 创建作业实例
    - 查看作业实例
    - 启动作业实例
    - 终止作业实例
    - 删除作业实例
    - 创建数据卷
    - 删除数据卷

参考分布式任务队列celery的设计，有几大关键子模块：
- user 
- broker
- worker
- backend
![](http://opkk27k9n.bkt.clouddn.com/18-4-16/1740887.jpg)

在本系统的设计中，云接口对应上述几个子模块相应地有：
- broker是云接口对外（对user）暴露的作业调度信息接入点。
- worker依据broker中的作业调度信息，执行作业并追踪作业的整个生命周期（即“监控作业进程”）。
- backend是由接口调用者指定的，被worker用于存储作业的相关信息。

### 4.1.3 实现方案
#### 4.1.3.1 运行环境
操作系统：主流Unix系统，包括Linux/OS X。
软件依赖：Docker, Python（包括Flask, Celery等第三方包）， Redis, MySQL, 阿里云容器服务。
#### 4.1.3.2 关键实现
TODO

## 4.2 核心调度层
核心调度层负责接收作业请求，调用核心算法模块进行实时定价和调度排期，并对基础设施层发出资源分配和调度指令。
### 4.2.1 设计目标
#### 4.2.1.1 系统环境和软件依赖

#### 4.2.1.2 设计职能目标
#### 4.2.1.3 设计性能目标

### 4.2.2 设计方案
#### 4.2.2.1 定价和调度算法描述
调度算法（Basic-Bacon算法）：
```
//Input: a new job request {W*T in [A, D), V}
//Output: accept or reject, and price if accepted
//procedure MAKE RESERVATION
//    for each t ∈ [A,D) do
//        demand(t) ← the demand estimate function at t
//        for each i ∈ [1, W] do
//            price(t)(i) ← the highest price p s.t.demand(t)(p) + promised[t] + i > Capacity
//        cost[t] ← price(t)(1) + price(t)(2) + ... + price(t)(W)
//    for each t ∈ [A, D-T] do
//        totalCost[t] ← cost[t] + ... + cost[t+T-1]
//    t* ← arg min(t ∈ [A, D-T])totalCost[t]
//    if V >= totalCost[t*] then
//        schedule the job to start at t*
//        return accept at cost totalCost[t*]
//    else
//        return reject
//end procedure
```
### 4.2.3 实现方案
#### 4.2.3.1 运行环境
操作系统：主流Unix系统，包括Linux/OS X。
软件依赖：Docker, Go, Redis, MySQL。

#### 4.2.3.2 关键实现

为了体现核心调度层的松耦合特性，本系统采用Go语言编写核心调度层（基础设施层的云接口使用Python语言编写），对上层实现了Celery消息协议，对下层（主要指核心应用层）提供了TCP接口处理作业请求。

核心调度层的代码结构如下：
```
src/goERACore
├── cloud
│   ├── interface.go
│   └── interface_test.go
├── core
│   ├── config.go
│   ├── data_test.go
│   ├── fetch_data.go
│   ├── logger.go
│   ├── model.go
│   ├── pred_model.go
│   ├── pred_test.go
│   ├── predictor.go
│   ├── pricing.go
│   ├── scheduling.go
│   ├── table.csv
│   └── utils.go
├── user
│   ├── dump.rdb
│   ├── rpcapi.go
│   ├── tcpapi.go
│   └── user_test.go
├── core.go
├── cloud.go
```

运行TCP API：`go run core.go`。
运行Celery Task Submitter: `go run cloud.go`。
因此，核心调度层的运行时至少有两个进程，其中TCP API负责接收TCP请求，解析请求内容，调用core包中的预测、定价、调度算法，将调度排期结果写到redis（通过redis提供的发布订阅机制实现）并返回给TCP客户端；Celery Task Submitter负责从redis中读取调度排期结果，并向云接口的作业调度消息代理发送符合Celery消息协议规范的消息。

## 4.3 核心应用层
核心应用层直接接受客户端的作业请求，并转发给核心调度层，从核心调度层获取响应后再向客户端发出响应，全程对于客户端来说是同步的（非异步），这就要求核心调度层具有非常高的响应速度。
### 4.3.1 设计目标
#### 4.3.1.1 系统环境和软件依赖
全面支持主流服务器操作系统；有限支持主流桌面操作系统。
#### 4.3.1.2 设计职能目标
#### 4.3.1.3 设计性能目标

### 4.3.2 设计方案
针对2.1节中提出的服务内容需求，本系统对其中一些非通用性服务给出如下设计方案：
- 核心应用服务
采用通用的HTTP服务器应用程序架构，并作为其他服务的管理者履行用户访问权限校验职能。
- 用户文件服务
采用websocket技术实现文件传输（上传和下载），采用基础设施层的NAS用于文件存储。用户文件服务独立于核心应用服务而存在，可以实现文件模块解耦，提升性能和稳定性。
- 用户作业监控服务
基于基础设施层的日志采集、性能监控等服务，核心应用层对这些监控数据进行权限管理和websocket传输等封装。此外，深度学习训练过程可视化则主要借由tensorboard实现，支持但不仅限于热门的tensorflow框架。
- 数据可视化服务
该服务将基于Jupyter Notebook及其扩展，构建中心化的服务器集群，在用户接入层提供数据可视化服务。
### 4.3.3 实现方案
#### 4.3.3.1 运行环境
操作系统：主流Unix系统，包括Linux/OS X。
软件依赖：Docker, Python（包括Flask, Tornado, SQLAlchemy等第三方包）， Redis, MySQL, Circle CI（可选）。
#### 4.3.3.2 关键实现
结合2.1节中需求分析及4.3.2节中的设计方案，本系统对核心应用层各服务的实现方案如下：
① 核心应用服务
基于Flask框架构建的高可扩展性的HTTP应用服务器，并采用uwsgi+supervisor+nginx方案进行部署。数据库方面，选用MySQL并采用了阿里云提供的读写分离架构，且编写了大量API以便查询、更新数据记录。
③ 用户文件服务
该服务基于Tornado构建，并将NAS挂载于Tornado所在的服务器主机上，提供文件上传、下载、浏览、搜索功能。
④ 用户作业监控服务
用户作业的终端日志存储在NAS数据盘的相应文件夹下，性能监控数据存储在独立的influxdb时序数据库中，通过Tornado搭建http/websocket API，供用户接入层访问这些数据。
⑤ 镜像定制发布服务
镜像定制方面，建立镜像文件（Dockerfile）代码仓库，采用Circle CI进行持续集成，主要流程如下：检测镜像文件更新，执行镜像构建，将镜像推送到镜像仓库。
镜像发布方面，所有镜像的信息封装在一个hash表，存储在Redis中。开发者通过写该数据结构完成镜像发布。

## 4.4 用户接入层
用户接入层面向用户提供系统接入，收集用户输入构造HTTP请求，发送给核心应用层，获取响应并解析给用户查看。
### 4.3.1 设计目标
#### 4.3.1.1 系统环境和软件依赖
全面支持主流桌面操作系统命令行，包括Windows的cmd，Linux和OS X等类Unix系统的terminal；
支持各大主流浏览器。
#### 4.3.1.2 设计职能目标
#### 4.3.1.3 设计性能目标

## 4.5 本章小结
本章介绍了本文所述系统的详细设计方案及其关键实现论述，本系统包括基础设施层、核心调度层、核心应用层和用户接入层，每一层各司其职，其中基础设施层和核心调度层构成了本文的核心研究内容。

下一章将对本章所设计实现的系统进行实际运行测试，并就测试结果进行分析，检验本系统的各项功能以及性能指标。

# 5. 测试与分析
## 5.1 功能测试
### 5.1.1 基础设施层
### 5.1.2 核心调度层
### 5.1.3 核心应用层
### 5.1.4 用户接入层
### 5.1.4 系统整体功能测试

## 5.2 性能测试
### 5.1.1 基础设施层
### 5.1.2 核心调度层
### 5.1.3 核心应用层
### 5.1.4 用户接入层
### 5.1.5 系统整体性能测试

## 5.3 本章小结

# 6. 总结
## 6.1 全文总结
## 6.2 课题展望

# 7. 致谢

# 9. 引用
- 虚拟化技术概述：http://dockone.io/article/1562
- Kubernetes介绍：https://kubernetes.io/
- Redis介绍：https://redis.io/, https://github.com/antirez/redis
- 自己动手写Docker 序 阿里云容器服务团队架构师易立
- Distributed task scheduler for computing environments：https://patents.google.com/patent/US20050268300A1/en
- Celery Protocol: http://docs.celeryproject.org/en/latest/internals/protocol.html